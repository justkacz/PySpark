{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7c2179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://L204LTP.sap-flex.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f28f945790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1992cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|  adam|  20|   5|  1000|\n",
      "|   ewa|  21|   3|  2000|\n",
      "|marcin|  30|  14|  3000|\n",
      "|   jan|  23|null|  4000|\n",
      "|  null|null|  13|  5000|\n",
      "| kasia|null|null|  null|\n",
      "|   ala|null|  34|  6000|\n",
      "+------+----+----+------+\n",
      "\n",
      "Wall time: 303 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pyspark=spark.read.csv('test1.csv', sep=';', header=True, inferSchema=True) # inferSchema=True -> adjusts dtype in each column (by default all dtypes are string)\n",
    "\n",
    "# or \n",
    "# df_pyspark=spark.read.option('header', 'true').csv('test1.csv').show()\n",
    "\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e815834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "487e5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "930985ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='adam', age=20, exp=5, salary=1000),\n",
       " Row(name='ewa', age=21, exp=3, salary=2000)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0fe6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| exp|\n",
      "+------+----+\n",
      "|  adam|   5|\n",
      "|   ewa|   3|\n",
      "|marcin|  14|\n",
      "|   jan|null|\n",
      "|  null|  13|\n",
      "| kasia|null|\n",
      "|   ala|  34|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting particular columns:\n",
    "df_pyspark.select(['Name', 'exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11723b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------------------+------------------+\n",
      "|summary|  name|              age|               exp|            salary|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "|  count|     6|                4|                 5|                 6|\n",
      "|   mean|  null|             23.5|              13.8|            3500.0|\n",
      "| stddev|  null|4.509249752822894|12.275992831539126|1870.8286933869706|\n",
      "|    min|  adam|               20|                 3|              1000|\n",
      "|    max|marcin|               30|                34|              6000|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cb9ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+------------+\n",
      "|  name| age| exp|salary|Add _2_years|\n",
      "+------+----+----+------+------------+\n",
      "|  adam|  20|   5|  1000|          22|\n",
      "|   ewa|  21|   3|  2000|          23|\n",
      "|marcin|  30|  14|  3000|          32|\n",
      "|   jan|  23|null|  4000|          25|\n",
      "|  null|null|  13|  5000|        null|\n",
      "| kasia|null|null|  null|        null|\n",
      "|   ala|null|  34|  6000|        null|\n",
      "+------+----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding columns in df:\n",
    "df_pyspark=df_pyspark.withColumn('Add _2_years', df_pyspark['age']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "925ef8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|  adam|  20|   5|  1000|\n",
      "|   ewa|  21|   3|  2000|\n",
      "|marcin|  30|  14|  3000|\n",
      "|   jan|  23|null|  4000|\n",
      "|  null|null|  13|  5000|\n",
      "| kasia|null|null|  null|\n",
      "|   ala|null|  34|  6000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns:\n",
    "df_pyspark=df_pyspark.drop('Add _2_years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e0f7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|   kasia|null|null|  null|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename thecolumns:\n",
    "df_pyspark=df_pyspark.withColumnRenamed('Name', 'New name')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70227e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null values:\n",
    "# na.drop(how='all' -> removes row only if all items are null/ how='any' -> default (empty parenthesis),removes row if any of the item is null)\n",
    "dfnat=df_pyspark.na.drop(how='any', thresh=3) #thresh => at least how many not null values must be present\n",
    "dfnat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a70e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+------+\n",
      "|New name| age|exp|salary|\n",
      "+--------+----+---+------+\n",
      "|    adam|  20|  5|  1000|\n",
      "|     ewa|  21|  3|  2000|\n",
      "|  marcin|  30| 14|  3000|\n",
      "|    null|null| 13|  5000|\n",
      "|     ala|null| 34|  6000|\n",
      "+--------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnas=df_pyspark.na.drop(how='any', subset=['exp']) #subset=> removes rows only where null value is in exp column\n",
    "dfnas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81cc2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|   kasia|null|null|  null|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff=df_pyspark.na.fill(\"missing_val\", subset='age')\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c077e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+\n",
      "|New name|age|exp|salary|\n",
      "+--------+---+---+------+\n",
      "|    adam| 20|  5|  1000|\n",
      "|     ewa| 21|  3|  2000|\n",
      "|  marcin| 30| 14|  3000|\n",
      "|     jan| 23|  0|  4000|\n",
      "| missing|  0| 13|  5000|\n",
      "|   kasia|  0|  0|     0|\n",
      "|     ala|  0| 34|  6000|\n",
      "+--------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.fillna({'New name':\"missing\",'age':0, 'exp':0, 'salary':0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fab96288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "|New name| age| exp|salary|age_inputer|exp_inputer|salary_inputer|\n",
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "|    adam|  20|   5|  1000|         20|          5|          1000|\n",
      "|     ewa|  21|   3|  2000|         21|          3|          2000|\n",
      "|  marcin|  30|  14|  3000|         30|         14|          3000|\n",
      "|     jan|  23|null|  4000|         23|         13|          4000|\n",
      "|    null|null|  13|  5000|         23|         13|          5000|\n",
      "|   kasia|null|null|  null|         23|         13|          3500|\n",
      "|     ala|null|  34|  6000|         23|         34|          6000|\n",
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imputer -> filling missing values with e.g. mean, median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(inputCols=['age', 'exp', 'salary'],\n",
    "               outputCols=[f'{c}_inputer' for c in [\"age\", \"exp\", \"salary\"]]).setStrategy('mean')\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "37c9c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n",
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter values:\n",
    "df_pyspark.filter('salary>=4000').show()\n",
    "# or\n",
    "df_pyspark.filter( df_pyspark['salary']>=4000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f1f3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|New name| exp|\n",
      "+--------+----+\n",
      "|     jan|null|\n",
      "|    null|  13|\n",
      "|     ala|  34|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('salary>=4000').select(['New name', 'exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69af0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple conditions:\n",
    "df_pyspark.filter((df_pyspark['salary']>=4000) & (df_pyspark['salary']<6000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c75c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+-----------+\n",
      "|New name|max(age)|max(exp)|max(salary)|\n",
      "+--------+--------+--------+-----------+\n",
      "|    adam|      20|       5|       1000|\n",
      "|    null|    null|      13|       5000|\n",
      "|     ala|    null|      34|       6000|\n",
      "|     ewa|      21|       3|       2000|\n",
      "|  marcin|      30|      14|       3000|\n",
      "|   kasia|    null|    null|       null|\n",
      "|     jan|      23|    null|       4000|\n",
      "+--------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby:\n",
    "df_pyspark.groupby('New name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "76a112de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|New name|count|\n",
      "+--------+-----+\n",
      "|    adam|    1|\n",
      "|    null|    1|\n",
      "|     ala|    1|\n",
      "|     ewa|    1|\n",
      "|  marcin|    1|\n",
      "|   kasia|    1|\n",
      "|     jan|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('New name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a3da4dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New name', 'age', 'exp', 'salary']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ML - simple example - salary prediction:\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "36b29a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+--------------------+\n",
      "|New name|age|exp|salary|Independent Features|\n",
      "+--------+---+---+------+--------------------+\n",
      "|    adam| 20|  5|  1000|          [20.0,5.0]|\n",
      "|     ewa| 21|  3|  2000|          [21.0,3.0]|\n",
      "|  marcin| 30| 14|  3000|         [30.0,14.0]|\n",
      "+--------+---+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# independent features are grouped:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureassembler=VectorAssembler(inputCols=['age', 'exp'], outputCol='Independent Features', handleInvalid='skip') # omits null\n",
    "                                                                                                                    #values\n",
    "indf=featureassembler.transform(df_pyspark)\n",
    "indf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a4a71a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|salary|Independent Features|\n",
      "+------+--------------------+\n",
      "|  1000|          [20.0,5.0]|\n",
      "|  2000|          [21.0,3.0]|\n",
      "|  3000|         [30.0,14.0]|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final=indf.select(['salary', 'Independent Features'])\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5af0ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|salary|Independent Features|\n",
      "+------+--------------------+\n",
      "|  1000|          [20.0,5.0]|\n",
      "|  3000|         [30.0,14.0]|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train, test=final.randomSplit([0.75, 0.25])\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35b71e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_26179adbaccb, numFeatures=2"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearRegression(featuresCol='Independent Features', labelCol='salary')\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e368f1d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-5bf19b0eaa4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "prediction=model.evaluate(test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa93751",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
