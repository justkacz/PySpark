{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1992cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|  adam|  20|   5|  1000|\n",
      "|   ewa|  21|   3|  2000|\n",
      "|marcin|  30|  14|  3000|\n",
      "|   jan|  23|null|  4000|\n",
      "|  null|null|  13|  5000|\n",
      "| kasia|null|null|  null|\n",
      "|   ala|null|  34|  6000|\n",
      "+------+----+----+------+\n",
      "\n",
      "Wall time: 602 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_pyspark=spark.read.csv('test1.csv', sep=';', header=True, inferSchema=True) # inferSchema=True -> adjusts dtype in each column (by default all dtypes are string)\n",
    "\n",
    "# or \n",
    "# df_pyspark=spark.read.option('header', 'true').csv('test1.csv').show()\n",
    "\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e815834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487e5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930985ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='adam', age=20, exp=5, salary=1000),\n",
       " Row(name='ewa', age=21, exp=3, salary=2000)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0fe6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| exp|\n",
      "+------+----+\n",
      "|  adam|   5|\n",
      "|   ewa|   3|\n",
      "|marcin|  14|\n",
      "|   jan|null|\n",
      "|  null|  13|\n",
      "| kasia|null|\n",
      "|   ala|  34|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting particular columns:\n",
    "df_pyspark.select(['Name', 'exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11723b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+------------------+------------------+\n",
      "|summary|  name|              age|               exp|            salary|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "|  count|     6|                4|                 5|                 6|\n",
      "|   mean|  null|             23.5|              13.8|            3500.0|\n",
      "| stddev|  null|4.509249752822894|12.275992831539126|1870.8286933869706|\n",
      "|    min|  adam|               20|                 3|              1000|\n",
      "|    max|marcin|               30|                34|              6000|\n",
      "+-------+------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb9ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+------------+\n",
      "|  name| age| exp|salary|Add _2_years|\n",
      "+------+----+----+------+------------+\n",
      "|  adam|  20|   5|  1000|          22|\n",
      "|   ewa|  21|   3|  2000|          23|\n",
      "|marcin|  30|  14|  3000|          32|\n",
      "|   jan|  23|null|  4000|          25|\n",
      "|  null|null|  13|  5000|        null|\n",
      "| kasia|null|null|  null|        null|\n",
      "|   ala|null|  34|  6000|        null|\n",
      "+------+----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding columns in df:\n",
    "df_pyspark=df_pyspark.withColumn('Add _2_years', df_pyspark['age']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925ef8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------+\n",
      "|  name| age| exp|salary|\n",
      "+------+----+----+------+\n",
      "|  adam|  20|   5|  1000|\n",
      "|   ewa|  21|   3|  2000|\n",
      "|marcin|  30|  14|  3000|\n",
      "|   jan|  23|null|  4000|\n",
      "|  null|null|  13|  5000|\n",
      "| kasia|null|null|  null|\n",
      "|   ala|null|  34|  6000|\n",
      "+------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns:\n",
    "df_pyspark=df_pyspark.drop('Add _2_years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0f7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|   kasia|null|null|  null|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename thecolumns:\n",
    "df_pyspark=df_pyspark.withColumnRenamed('Name', 'New name')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70227e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop null values:\n",
    "# na.drop(how='all' -> removes row only if all items are null/ how='any' -> default (empty parenthesis),removes row if any of the item is null)\n",
    "dfnat=df_pyspark.na.drop(how='any', thresh=3) #thresh => at least how many not null values must be present\n",
    "dfnat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a70e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+------+\n",
      "|New name| age|exp|salary|\n",
      "+--------+----+---+------+\n",
      "|    adam|  20|  5|  1000|\n",
      "|     ewa|  21|  3|  2000|\n",
      "|  marcin|  30| 14|  3000|\n",
      "|    null|null| 13|  5000|\n",
      "|     ala|null| 34|  6000|\n",
      "+--------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnas=df_pyspark.na.drop(how='any', subset=['exp']) #subset=> removes rows only where null value is in exp column\n",
    "dfnas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81cc2731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|    adam|  20|   5|  1000|\n",
      "|     ewa|  21|   3|  2000|\n",
      "|  marcin|  30|  14|  3000|\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|   kasia|null|null|  null|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff=df_pyspark.na.fill(\"missing_val\", subset='age')\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c077e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+\n",
      "|New name|age|exp|salary|\n",
      "+--------+---+---+------+\n",
      "|    adam| 20|  5|  1000|\n",
      "|     ewa| 21|  3|  2000|\n",
      "|  marcin| 30| 14|  3000|\n",
      "|     jan| 23|  0|  4000|\n",
      "| missing|  0| 13|  5000|\n",
      "|   kasia|  0|  0|     0|\n",
      "|     ala|  0| 34|  6000|\n",
      "+--------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.fillna({'New name':\"missing\",'age':0, 'exp':0, 'salary':0}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab96288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "|New name| age| exp|salary|age_inputer|exp_inputer|salary_inputer|\n",
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "|    adam|  20|   5|  1000|         20|          5|          1000|\n",
      "|     ewa|  21|   3|  2000|         21|          3|          2000|\n",
      "|  marcin|  30|  14|  3000|         30|         14|          3000|\n",
      "|     jan|  23|null|  4000|         23|         13|          4000|\n",
      "|    null|null|  13|  5000|         23|         13|          5000|\n",
      "|   kasia|null|null|  null|         23|         13|          3500|\n",
      "|     ala|null|  34|  6000|         23|         34|          6000|\n",
      "+--------+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imputer -> filling missing values with e.g. mean, median\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(inputCols=['age', 'exp', 'salary'],\n",
    "               outputCols=[f'{c}_inputer' for c in [\"age\", \"exp\", \"salary\"]]).setStrategy('mean')\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c9c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n",
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "|     ala|null|  34|  6000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter values:\n",
    "df_pyspark.filter('salary>=4000').show()\n",
    "# or\n",
    "df_pyspark.filter( df_pyspark['salary']>=4000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f1f3743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|New name| exp|\n",
      "+--------+----+\n",
      "|     jan|null|\n",
      "|    null|  13|\n",
      "|     ala|  34|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('salary>=4000').select(['New name', 'exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69af0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+------+\n",
      "|New name| age| exp|salary|\n",
      "+--------+----+----+------+\n",
      "|     jan|  23|null|  4000|\n",
      "|    null|null|  13|  5000|\n",
      "+--------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple conditions:\n",
    "df_pyspark.filter((df_pyspark['salary']>=4000) & (df_pyspark['salary']<6000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c75c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+-----------+\n",
      "|New name|max(age)|max(exp)|max(salary)|\n",
      "+--------+--------+--------+-----------+\n",
      "|    adam|      20|       5|       1000|\n",
      "|    null|    null|      13|       5000|\n",
      "|     ala|    null|      34|       6000|\n",
      "|     ewa|      21|       3|       2000|\n",
      "|  marcin|      30|      14|       3000|\n",
      "|   kasia|    null|    null|       null|\n",
      "|     jan|      23|    null|       4000|\n",
      "+--------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby:\n",
    "df_pyspark.groupby('New name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76a112de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|New name|count|\n",
      "+--------+-----+\n",
      "|    adam|    1|\n",
      "|    null|    1|\n",
      "|     ala|    1|\n",
      "|     ewa|    1|\n",
      "|  marcin|    1|\n",
      "|   kasia|    1|\n",
      "|     jan|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('New name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3da4dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New name', 'age', 'exp', 'salary']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ML - simple example - salary prediction:\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36b29a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+--------------------+\n",
      "|New name|age|exp|salary|Independent Features|\n",
      "+--------+---+---+------+--------------------+\n",
      "|    adam| 20|  5|  1000|          [20.0,5.0]|\n",
      "|     ewa| 21|  3|  2000|          [21.0,3.0]|\n",
      "|  marcin| 30| 14|  3000|         [30.0,14.0]|\n",
      "+--------+---+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# independent features are grouped:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureassembler=VectorAssembler(inputCols=['age', 'exp'], outputCol='Independent Features', handleInvalid='skip') # omits null\n",
    "                                                                                                                    #values\n",
    "indf=featureassembler.transform(df_pyspark)\n",
    "indf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4a71a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|salary|Independent Features|\n",
      "+------+--------------------+\n",
      "|  1000|          [20.0,5.0]|\n",
      "|  2000|          [21.0,3.0]|\n",
      "|  3000|         [30.0,14.0]|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final=indf.select(['salary', 'Independent Features'])\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5af0ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|salary|Independent Features|\n",
      "+------+--------------------+\n",
      "|  1000|          [20.0,5.0]|\n",
      "|  3000|         [30.0,14.0]|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train, test=final.randomSplit([0.75, 0.25])\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35b71e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression(featuresCol='Independent Features', labelCol='salary', regParam=0.3, elasticNetParam=0.8)\n",
    "model_f=model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07d3ca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([99.973, 111.0811])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d9bbefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1554.5955843546815"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e368f1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----------------+\n",
      "|salary|Independent Features|       prediction|\n",
      "+------+--------------------+-----------------+\n",
      "|  2000|          [21.0,3.0]|878.0807686880519|\n",
      "+------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkaczmarek\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prediction=model_f.evaluate(test)\n",
    "prediction.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82e04fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1121.9192313119481, 1258702.7615875925)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.meanAbsoluteError, prediction.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
